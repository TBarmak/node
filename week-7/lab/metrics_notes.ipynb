{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week7lab_notes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IGOkh4T9xug"
      },
      "source": [
        "# **Classification Metrics: Tools (Aside from Accuracy) for Assessing ML Models**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ET4NkS4Jj9"
      },
      "source": [
        "# Import the following packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Numpy random seed for consistency\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "np.random.seed(123) #use this random \"seed\" so that we can all get the same synthetic data!\n",
        "\n",
        "# To model normal distribution\n",
        "from scipy.stats import norm\n",
        "\n",
        "# To make data\n",
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40fMDGy2-fyv"
      },
      "source": [
        "## Let's begin by making synthetic data with 2 features (to be used for classification as a 1 or 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_APxI1Et4TDk"
      },
      "source": [
        "#Make data set with 3000 observations\n",
        "n = 3000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Cl2nZ84YqX"
      },
      "source": [
        "centers = [[9.5, 0], [10.5, 0]] # Define the coordinates to center our blobs (x,y)\n",
        "X, y = make_blobs(n_samples=n, centers=centers, cluster_std=0.4, random_state=7)\n",
        "data = pd.DataFrame(X, columns=['feature1','feature2']) # Rename the feature columns (like x and y; coordinates to be used to classify points as 0 or 1)\n",
        "data['target'] = y.astype('str') # Convert dtype to help w/ viz\n",
        "\n",
        "data.head() #view the first few rows "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lmZyIQb4jH-"
      },
      "source": [
        "#View the shape of our synthetic data, and the frequencies of each class (Hint: value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ESS5qEy_Cf7"
      },
      "source": [
        "As you can see, the \"class frequencies\" of 0 and 1 observations depict a 50-50 split, meaning that half of our data is 1's and half of our data is 0's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbArmS4__hKy"
      },
      "source": [
        "**Below is a pre-made classifier (common classifiers we have/may learn are regression, Decision Trees, K Nearest Neighbors, etc.). This classifier will make the predictions of 0's and 1's based upon training and testing data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8K2G0jB470f"
      },
      "source": [
        "class BoundaryClassifier():\n",
        "    def __init__(self):\n",
        "        from scipy.stats import norm\n",
        "        self.name = 'Classify observations on 1D boundary'\n",
        "    \n",
        "    def fit(self, X_train, y_train, x_boundary=None):\n",
        "        self.boundary = x_boundary\n",
        "        \n",
        "    def predict(self, X_test):\n",
        "        b = self.boundary\n",
        "        x = X_test.feature1\n",
        "        y_pred =  (x > b).astype(np.int) #boundary, b, a threshold we can use to determine if observation is a 0 or a 1\n",
        "        return y_pred\n",
        "    \n",
        "    def predict_proba(self, X_test): #the predicted probability\n",
        "        b = self.boundary\n",
        "        x = X_test.feature1\n",
        "        \n",
        "        # Use the normal distribution to model probabilities\n",
        "        y_pred_proba = ((x-b)/0.4).apply(norm.cdf)\n",
        "        return y_pred_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiW_M-byAmvT"
      },
      "source": [
        "**1. As learned, split your data into training and testing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx3SWP7JOXnV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udp7o23oAwgq"
      },
      "source": [
        "**2.Employ the Classifier \"BoundaryClassifier()\" to fit the model to the data and predict the 0 and 1 classes. Hint: an extra input is needed in clf.fit(), called x_boundary. Set this boundary/threshold=10, which is the threshold we can use to determine if a point is a 0 or 1 (threshold determined for this specific synthetic dataset)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL0EqTpsOYU7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqfgMfTBgBq"
      },
      "source": [
        "**3.Create a data frame to view the actual class, predicted class (from model), and predicted probability ('y_pred_proba'), from BoundaryClassifier()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1thgKwEOt06"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0l9HuA5CNIh"
      },
      "source": [
        "## **Classification Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh7atiV5CZ7v"
      },
      "source": [
        "1. Compute the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f4D_kOC5Q_F"
      },
      "source": [
        "##ACCURACY SCORE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrrIiRAdCi0e"
      },
      "source": [
        "2. Create a confusion matrix to model the true positives, true negatives, false positives, and false negatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcIwOi8S5VGL"
      },
      "source": [
        "##CONFUSION MATRIX\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO7Qlu4j5bX7"
      },
      "source": [
        "#code to turn outputted matrix into a dataframe\n",
        "def custom_confusion_matrix(y_test_, y_pred_proba_, alpha=0.5, output='dataframe'):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "        cm = custom_confusion_matrix(y_test, y_pred_proba, output = 'dataframe')\n",
        "        tn, fp, fn, tp = custom_confusion_matrix(y_test, y_pred_proba, output = 'rates')\n",
        "\n",
        "    Params:\n",
        "        alpha: Threshold probability for classification (default = 0.5)\n",
        "        output: One of 'dataframe', 'rates', or 'array'\n",
        "    \"\"\"\n",
        "    y_pred_ = (y_pred_proba_ >  alpha).map({True:1,False:0})\n",
        "    cf_mat_ = confusion_matrix(y_test_, y_pred_)\n",
        "    if output == 'dataframe':\n",
        "        return pd.DataFrame(cf_mat_, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
        "    elif output == 'rates':\n",
        "        return cf_mat_.ravel()\n",
        "    else:\n",
        "        return cf_mat_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjUEE8Nt5cVO"
      },
      "source": [
        "cm = custom_confusion_matrix(y_test, y_pred_proba, output = 'dataframe')\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDJzLix5gHZ"
      },
      "source": [
        "#assigning values to corresponding tn, fp, fn, tp measures\n",
        "tn, fp, fn, tp = custom_confusion_matrix(y_test, y_pred_proba, output = 'rates')\n",
        "tn, fp, fn, tp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j9nS1fgEGkQ"
      },
      "source": [
        "3. Compute the Sensitivity, Specificity, Precision, and F-1 Scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1DwLHO05jWC"
      },
      "source": [
        "##SENSITIVITY\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMlB7mzc5oRU"
      },
      "source": [
        "##SPECIFICITY\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBY1xmsc5qBm"
      },
      "source": [
        "##PRECISION\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOdcOTFk5vzv"
      },
      "source": [
        "##F-1 SCORE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usFg0ZBzENc-"
      },
      "source": [
        "# What does each of these metrics mean in context of the classes in this model?\n"
      ]
    }
  ]
}